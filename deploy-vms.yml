---
# VM Deployment Playbook for Proxmox
#
# This playbook uses SSH + qm CLI commands for deployment because:
# 1. qm importdisk has NO API equivalent - must use CLI
# 2. Complex disk configurations are simpler via atomic qm set commands
# 3. Storage type detection (pvesm status) is CLI-only
#
# For API-based operations (monitoring, IP detection), see:
# - scripts/proxmox_get_vm_ip.py (uses Proxmox API)
# - generate-inventory.sh (uses API for IP detection)
#
# See API_USAGE.md for detailed explanation of hybrid approach.

- name: Deploy OpenSUSE Virtual Machines on Proxmox
  hosts: proxmox_host
  gather_facts: false

  vars_files:
    - vars/vm_config.yml

  tasks:

    - name: Detect storage type for the configured pool
      ansible.builtin.shell: |
        pvesm status -storage {{ storage_pool }} | awk 'NR==2 {print $2}'
      register: storage_type_result
      changed_when: false
      failed_when: false

    - name: Set storage type fact
      ansible.builtin.set_fact:
        storage_type: "{{ storage_type_result.stdout | trim }}"

    - name: Display detected storage type
      ansible.builtin.debug:
        msg: "Detected storage type for '{{ storage_pool }}': {{ storage_type }}"

    - name: Check if custom OpenSUSE image exists
      ansible.builtin.stat:
        path: "{{ opensuse_image_path }}"
      register: image_check

    - name: Fail if image doesn't exist
      ansible.builtin.fail:
        msg: "OpenSUSE image not found at {{ opensuse_image_path }}. Please build it first using: cd kiwi && ./build-image.sh"
      when: not image_check.stat.exists

    - name: Check if VMs or Containers with these IDs already exist
      ansible.builtin.shell: |
        IS_VM=""
        IS_CT=""
        if qm status {{ item.vmid }} >/dev/null 2>&1; then
          IS_VM="yes"
        fi
        if pct status {{ item.vmid }} >/dev/null 2>&1; then
          IS_CT="yes"
        fi

        if [ -n "$IS_VM" ] || [ -n "$IS_CT" ]; then
          if [ -n "$IS_VM" ]; then
            echo "exists_vm"
          else
            echo "exists_ct"
          fi
        else
          echo "absent"
        fi
      loop: "{{ vms }}"
      register: vm_exists
      changed_when: false
      failed_when: false

    - name: Display warning for existing VMs
      ansible.builtin.debug:
        msg: |
          ⚠️  WARNING: VM {{ item.item.vmid }} ({{ item.item.name }}) already exists!

          This VM will be SKIPPED during deployment.

          If you want to recreate this VM:
          1. Run: make cleanup-vms CONFIRM_DELETE=true
          2. Or manually: ssh root@{{ proxmox_api_host }} "qm destroy {{ item.item.vmid }}"
          3. Then run: make deploy
      loop: "{{ vm_exists.results }}"
      when: item.stdout == "exists_vm"

    - name: Fail if VM ID is already used by a Container
      ansible.builtin.fail:
        msg: |
          ❌ ERROR: Cannot create VM {{ item.item.vmid }} ({{ item.item.name }})

          This ID is already used by an LXC Container (CT {{ item.item.vmid }}).

          To fix this, either:
          1. Remove the container: ssh root@{{ proxmox_api_host }} "pct destroy {{ item.item.vmid }}"
          2. Or change VM IDs in your .env file:
             - Edit .env and set VM{{ loop.index }}_VMID to a different ID
             - Run: make deploy
      loop: "{{ vm_exists.results }}"
      when: item.stdout == "exists_ct"

    - name: Count VMs that already exist
      ansible.builtin.set_fact:
        existing_vms_count: "{{ vm_exists.results | selectattr('stdout', 'equalto', 'exists_vm') | list | length }}"

    - name: Pause if VMs already exist (allow user to cancel)
      ansible.builtin.pause:
        prompt: |

          ⚠️  {{ existing_vms_count }} VM(s) already exist and will be SKIPPED.

          Existing VMs will NOT be modified or recreated.
          Only NEW VMs will be created.

          To cleanup and recreate all VMs:
            Press Ctrl+C, then run: make cleanup-vms CONFIRM_DELETE=true

          To continue with partial deployment:
            Press ENTER to continue
        seconds: 15
      when: existing_vms_count | int > 0

    - name: Create VMs with basic configuration using qm
      ansible.builtin.shell: |
        qm create {{ item.item.vmid }} \
          --name {{ item.item.name }} \
          --memory {{ item.item.memory | default(vm_default_memory) }} \
          --cores {{ item.item.cores | default(vm_default_cores) }} \
          --sockets {{ item.item.sockets | default(vm_default_sockets) }} \
          --cpu {{ vm_cpu_type }} \
          --ostype l26 \
          --scsihw virtio-scsi-single \
          --agent enabled=1
      loop: "{{ vm_exists.results }}"
      when: item.stdout == "absent"
      changed_when: true

    - name: Import and configure OS disk (ZFS storage - thin provisioned)
      ansible.builtin.shell: |
        # Import disk image (thin provisioning controlled by ZFS storage config)
        qm importdisk {{ item.vmid }} {{ opensuse_image_path }} {{ storage_pool }}

        # Attach OS disk as scsi0
        qm set {{ item.vmid }} --scsi0 {{ storage_pool }}:vm-{{ item.vmid }}-disk-0,discard=on,size=50G

        # Set boot disk
        qm set {{ item.vmid }} --boot order=scsi0
      loop: "{{ vms }}"
      when: storage_type == "zfspool"
      changed_when: true
      args:
        executable: /bin/bash

    - name: Import and configure OS disk (LVM-thin storage - thin provisioned)
      ansible.builtin.shell: |
        # Import disk image (LVM-thin is thin provisioned by the storage backend)
        qm importdisk {{ item.vmid }} {{ opensuse_image_path }} {{ storage_pool }}

        # Attach OS disk as scsi0
        qm set {{ item.vmid }} --scsi0 {{ storage_pool }}:vm-{{ item.vmid }}-disk-0,discard=on,size=50G,ssd=1,cache=writeback

        # Set boot disk
        qm set {{ item.vmid }} --boot order=scsi0
      loop: "{{ vms }}"
      when: storage_type == "lvmthin"
      changed_when: true
      args:
        executable: /bin/bash

    - name: Import and configure OS disk (Directory/other storage - thin provisioned)
      ansible.builtin.shell: |
        # Import disk image as qcow2 (thin provisioned by default)
        qm importdisk {{ item.vmid }} {{ opensuse_image_path }} {{ storage_pool }} --format qcow2

        # Attach OS disk as scsi0
        qm set {{ item.vmid }} --scsi0 {{ storage_pool }}:vm-{{ item.vmid }}-disk-0,discard=on,size=50G,ssd=1,cache=writeback

        # Set boot disk
        qm set {{ item.vmid }} --boot order=scsi0
      loop: "{{ vms }}"
      when: storage_type not in ["zfspool", "lvmthin"]
      changed_when: true
      args:
        executable: /bin/bash

    - name: Create and attach data disks (ZFS storage - thin provisioned)
      ansible.builtin.shell: |
        for i in {1..4}; do
          # Thin provisioning controlled by ZFS storage pool configuration
          # Format: pool:size (size in GB without G suffix)
          qm set {{ item.vmid }} --scsi$i {{ storage_pool }}:{{ data_disk_size | regex_replace('G$', '') }},discard=on
        done
      loop: "{{ vms }}"
      when: storage_type == "zfspool"
      changed_when: true
      args:
        executable: /bin/bash

    - name: Create and attach data disks (LVM-thin storage - thin provisioned)
      ansible.builtin.shell: |
        for i in {1..4}; do
          # Use format=raw for LVM-thin, qcow2 for others (both support thin provisioning)
          qm set {{ item.vmid }} --scsi$i {{ storage_pool }}:{{ data_disk_size }},discard=on,cache=writeback
        done
      loop: "{{ vms }}"
      when: storage_type == "lvmthin"
      changed_when: true
      args:
        executable: /bin/bash

    - name: Create and attach data disks (Directory/other storage - thin provisioned)
      ansible.builtin.shell: |
        for i in {1..4}; do
          # qcow2 format is thin provisioned by default
          qm set {{ item.vmid }} --scsi$i {{ storage_pool }}:{{ data_disk_size }},format=qcow2,discard=on,cache=writeback
        done
      loop: "{{ vms }}"
      when: storage_type not in ["zfspool", "lvmthin"]
      changed_when: true
      args:
        executable: /bin/bash

    - name: Create and attach Ceph mon disk (ZFS storage - thin provisioned)
      ansible.builtin.shell: |
        # Thin provisioning controlled by ZFS storage pool configuration
        qm set {{ item.vmid }} --scsi5 {{ storage_pool }}:{{ mon_disk_size | default('100G') | regex_replace('G$', '') }},discard=on
      loop: "{{ vms }}"
      when: storage_type == "zfspool"
      changed_when: true
      args:
        executable: /bin/bash

    - name: Create and attach Ceph mon disk (LVM-thin storage - thin provisioned)
      ansible.builtin.shell: |
        # LVM-thin uses raw format, thin provisioned by the storage backend
        qm set {{ item.vmid }} --scsi5 {{ storage_pool }}:{{ mon_disk_size | default('100G') }},discard=on,cache=writeback
      loop: "{{ vms }}"
      when: storage_type == "lvmthin"
      changed_when: true
      args:
        executable: /bin/bash

    - name: Create and attach Ceph mon disk (Directory/other storage - thin provisioned)
      ansible.builtin.shell: |
        # qcow2 format is thin provisioned by default
        qm set {{ item.vmid }} --scsi5 {{ storage_pool }}:{{ mon_disk_size | default('100G') }},format=qcow2,discard=on,cache=writeback
      loop: "{{ vms }}"
      when: storage_type not in ["zfspool", "lvmthin"]
      changed_when: true
      args:
        executable: /bin/bash

    - name: Configure network interfaces
      ansible.builtin.shell: |
        # Private bridge network
        qm set {{ item.vmid }} --net0 virtio,bridge={{ private_bridge }},firewall=0
        
        # Public/separate network
        qm set {{ item.vmid }} --net1 virtio,bridge={{ public_bridge }},firewall=0
      loop: "{{ vms }}"
      changed_when: true
      args:
        executable: /bin/bash

    - name: Configure cloud-init with hostname and credentials
      ansible.builtin.shell: |
        # Configure cloud-init with hostname, user, and password
        qm set {{ item.vmid }} \
          --ciuser root \
          --cipassword '{{ vm_root_password | default("opensuse") }}' \
          --searchdomain local \
          --nameserver 8.8.8.8

        # Create cloud-init drive
        qm set {{ item.vmid }} --ide2 {{ storage_pool }}:cloudinit
      loop: "{{ vms }}"
      changed_when: true
      failed_when: false
      args:
        executable: /bin/bash

    - name: Configure VM performance settings
      ansible.builtin.shell: |
        qm set {{ item.vmid }} \
          --numa 1 \
          --balloon 0 \
          --onboot {{ item.onboot | default(1) }}
      loop: "{{ vms }}"
      changed_when: true
      args:
        executable: /bin/bash

    - name: Start VMs
      ansible.builtin.shell: |
        qm start {{ item.vmid }}
      loop: "{{ vms }}"
      when: auto_start | default(true)
      changed_when: true

    - name: Display VM deployment summary
      ansible.builtin.debug:
        msg: |
          ===============================================
          VM Deployed: {{ item.name }}
          ===============================================
          VMID: {{ item.vmid }}
          Memory: {{ item.memory | default(vm_default_memory) }} MB
          Cores: {{ item.cores | default(vm_default_cores) }}
          OS Disk: 50GB thin provisioned on {{ storage_pool }}
          Data Disks: 4 x {{ data_disk_size }} on {{ storage_pool }} (UNFORMATTED - for Ceph OSD)
          Mon Disk: {{ mon_disk_size | default('100G') }} on {{ storage_pool }} (UNFORMATTED - for Ceph MON)
          Network 1: {{ private_bridge }} (Private)
          Network 2: {{ public_bridge }} (Public)
          {% if item.ip is defined %}Target IP: {{ item.ip }}{% endif %}

          Next steps:
          1. Update inventory-vms.ini with actual VM IPs
          2. Run: make configure
          ===============================================
      loop: "{{ vms }}"
